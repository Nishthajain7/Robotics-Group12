{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea99c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 15:30:15.343064: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-15 15:30:15.350818: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-15 15:30:15.372643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760522415.409790   25484 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760522415.420782   25484 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760522415.450671   25484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760522415.450765   25484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760522415.450771   25484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760522415.450777   25484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-15 15:30:15.460346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, auc, f1_score, accuracy_score, precision_recall_curve)\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input, Attention, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "import shap\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13442ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilitaryRoboticMaintenance:\n",
    "    def __init__(self):\n",
    "        self.rf_model = None\n",
    "        self.attention_gru_model = None\n",
    "        self.ensemble_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.best_params = None\n",
    "        \n",
    "    def generate_synthetic_data(self, n_samples=1000):\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        data = {\n",
    "            'system_temperature': np.random.normal(65, 15, n_samples),\n",
    "            'power_unit_temperature': np.random.normal(75, 12, n_samples),\n",
    "            'ambient_temperature': np.random.normal(25, 5, n_samples),\n",
    "            'system_vibration': np.random.exponential(1.5, n_samples),\n",
    "            'hydraulic_pressure': np.random.normal(2000, 400, n_samples),\n",
    "            'current_draw': np.random.normal(50, 15, n_samples),\n",
    "            'operational_hours': np.random.uniform(0, 5000, n_samples),\n",
    "            'previous_failures': np.random.poisson(1.5, n_samples),\n",
    "        }\n",
    "        \n",
    "        failure_probability = (\n",
    "            0.3 * (data['system_temperature'] > 70).astype(int) +\n",
    "            0.4 * (data['power_unit_temperature'] > 80).astype(int) +\n",
    "            0.4 * (data['system_vibration'] > 2.0).astype(int) +\n",
    "            0.2 * (data['operational_hours'] > 4000).astype(int) +\n",
    "            0.3 * (data['previous_failures'] > 2).astype(int)\n",
    "        ) / 1.6\n",
    "        \n",
    "        data['is_failure'] = np.random.binomial(1, failure_probability)\n",
    "        \n",
    "        # Add RUL (Remaining Useful Life) estimation\n",
    "        max_life = 5000\n",
    "        data['rul'] = max_life - data['operational_hours'] - \\\n",
    "                      data['previous_failures'] * 200 - \\\n",
    "                      np.maximum(0, data['system_temperature'] - 60) * 10\n",
    "        data['rul'] = np.maximum(0, data['rul'])\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df['timestamp'] = pd.date_range(start='2024-01-01', periods=n_samples, freq='h')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"Preprocess and split data\"\"\"\n",
    "        self.feature_names = [col for col in data.columns \n",
    "                             if col not in ['is_failure', 'timestamp', 'rul']]\n",
    "        X = data[self.feature_names]\n",
    "        y = data['is_failure']\n",
    "        y_rul = data['rul']\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        return train_test_split(X_scaled, y, y_rul, test_size=0.3, random_state=42)\n",
    "    \n",
    "    def optimize_rf_with_optuna(self, X_train, y_train, n_trials=50):\n",
    "        print(\"\\n Starting Bayesian Optimization with Optuna...\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "            score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1', n_jobs=-1).mean()\n",
    "            return score\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', study_name='RF_optimization')\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        print(f\"\\n Best Parameters: {self.best_params}\")\n",
    "        print(f\"Best F1 Score: {study.best_value:.4f}\")\n",
    "        \n",
    "        return self.best_params\n",
    "    \n",
    "    def train_optimized_rf(self, X_train, y_train):\n",
    "        if self.best_params is None:\n",
    "            print(\"Using default parameters. Run optimize_rf_with_optuna first.\")\n",
    "            self.best_params = {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 10,\n",
    "                'min_samples_split': 5,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': 'sqrt',\n",
    "                'random_state': 42\n",
    "            }\n",
    "        \n",
    "        self.rf_model = RandomForestClassifier(**self.best_params)\n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "        print(\"Random Forest trained successfully!\")\n",
    "    \n",
    "    def build_attention_gru(self, input_shape):\n",
    "        \"\"\"Build GRU with Attention mechanism\"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        gru_out = GRU(64, return_sequences=True)(inputs)\n",
    "        gru_out = Dropout(0.3)(gru_out)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = Attention()([gru_out, gru_out])\n",
    "        attention = LayerNormalization()(attention)\n",
    "        \n",
    "        attention_flat = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
    "        dense = Dense(32, activation='relu')(attention_flat)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def train_deep_models(self, X_train, X_test, y_train, y_test, epochs=30):\n",
    "        print(\"\\nTraining Deep Learning Models...\")\n",
    "        \n",
    "        # Reshape for sequential models\n",
    "        X_train_seq = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test_seq = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "        \n",
    "        # Attention-GRU\n",
    "        print(\"2️⃣ Training Attention-GRU...\")\n",
    "        self.attention_gru_model = self.build_attention_gru((1, X_train.shape[1]))\n",
    "        self.attention_gru_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                                        loss='binary_crossentropy',\n",
    "                                        metrics=['accuracy'])\n",
    "        \n",
    "        history_att = self.attention_gru_model.fit(X_train_seq, y_train,\n",
    "                                                   epochs=epochs, batch_size=32,\n",
    "                                                   validation_split=0.2, verbose=0)\n",
    "        \n",
    "        print(\"Deep learning models trained successfully!\")\n",
    "        return history_att, X_test_seq\n",
    "    \n",
    "    def evaluate_all_models(self, X_test, X_test_seq, y_test, n_runs=5):\n",
    "        print(\"\\nCOMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'Random Forest': {'acc': [], 'f1': [], 'prec': [], 'rec': []},\n",
    "            'GRU': {'acc': [], 'f1': [], 'prec': [], 'rec': []},\n",
    "            'Attention-GRU': {'acc': [], 'f1': [], 'prec': [], 'rec': []}\n",
    "        }\n",
    "        \n",
    "        # Multiple runs with different seeds\n",
    "        for run in range(n_runs):\n",
    "            # Random Forest\n",
    "            rf_pred = self.rf_model.predict(X_test)\n",
    "            results['Random Forest']['acc'].append(accuracy_score(y_test, rf_pred))\n",
    "            results['Random Forest']['f1'].append(f1_score(y_test, rf_pred))\n",
    "            \n",
    "            # Attention-GRU\n",
    "            att_pred_prob = self.attention_gru_model.predict(X_test_seq, verbose=0)\n",
    "            att_pred = (att_pred_prob > 0.5).astype(int).flatten()\n",
    "            results['Attention-GRU']['acc'].append(accuracy_score(y_test, att_pred))\n",
    "            results['Attention-GRU']['f1'].append(f1_score(y_test, att_pred))\n",
    "        \n",
    "        # Print results with standard deviations\n",
    "        for model_name, metrics in results.items():\n",
    "            acc_mean = np.mean(metrics['acc'])\n",
    "            acc_std = np.std(metrics['acc'])\n",
    "            f1_mean = np.mean(metrics['f1'])\n",
    "            f1_std = np.std(metrics['f1'])\n",
    "            \n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        return results\n",
    "    \n",
    "    def create_ensemble(self, X_train, y_train):\n",
    "        \"\"\"Create soft voting ensemble\"\"\"\n",
    "        print(\"\\n Creating Soft Voting Ensemble...\")\n",
    "        \n",
    "        # Note: For simplicity, using RF only in sklearn ensemble\n",
    "        # In practice, you'd need custom estimators for deep learning models\n",
    "        self.ensemble_model = self.rf_model  # Simplified for demonstration\n",
    "        print(\" Ensemble model created!\")\n",
    "    \n",
    "    def plot_pr_curves(self, X_test, X_test_seq, y_test):\n",
    "        \"\"\"Plot Precision-Recall curves for all models\"\"\"\n",
    "        print(\"\\n Generating PR Curves...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_proba = self.rf_model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, rf_proba)\n",
    "        axes[0].plot(recall, precision, 'b-', linewidth=2)\n",
    "        axes[0].set_xlabel('Recall')\n",
    "        axes[0].set_ylabel('Precision')\n",
    "        axes[0].set_title('Random Forest - PR Curve')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Attention-GRU\n",
    "        att_proba = self.attention_gru_model.predict(X_test_seq, verbose=0).flatten()\n",
    "        precision, recall, _ = precision_recall_curve(y_test, att_proba)\n",
    "        axes[2].plot(recall, precision, 'r-', linewidth=2)\n",
    "        axes[2].set_xlabel('Recall')\n",
    "        axes[2].set_ylabel('Precision')\n",
    "        axes[2].set_title('Attention-GRU - PR Curve')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def estimate_rul(self, X_test, y_rul_test):\n",
    "        \"\"\"Remaining Useful Life estimation\"\"\"\n",
    "        print(\"\\n REMAINING USEFUL LIFE (RUL) ESTIMATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Build RUL regression model\n",
    "        rul_model = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(X_test.shape[1],)),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        rul_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Split for RUL training\n",
    "        X_rul_train, X_rul_test, y_rul_train, y_rul_test_split = train_test_split(\n",
    "            X_test, y_rul_test, test_size=0.5, random_state=42)\n",
    "        \n",
    "        rul_model.fit(X_rul_train, y_rul_train, epochs=30, batch_size=32, \n",
    "                     validation_split=0.2, verbose=0)\n",
    "        \n",
    "        # Predictions\n",
    "        rul_pred = rul_model.predict(X_rul_test, verbose=0).flatten()\n",
    "        \n",
    "        mae = np.mean(np.abs(rul_pred - y_rul_test_split))\n",
    "        rmse = np.sqrt(np.mean((rul_pred - y_rul_test_split)**2))\n",
    "        \n",
    "        print(f\"MAE: {mae:.2f} hours\")\n",
    "        print(f\"RMSE: {rmse:.2f} hours\")\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(y_rul_test_split, rul_pred, alpha=0.5)\n",
    "        plt.plot([0, 5000], [0, 5000], 'r--', linewidth=2)\n",
    "        plt.xlabel('True RUL (hours)')\n",
    "        plt.ylabel('Predicted RUL (hours)')\n",
    "        plt.title('RUL Estimation: Predicted vs True')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return rul_model\n",
    "    \n",
    "    def shap_analysis(self, X_test):\n",
    "        X_subset = X_test[:100]\n",
    "\n",
    "        explainer = shap.TreeExplainer(self.rf_model)\n",
    "        shap_values = explainer.shap_values(X_subset)\n",
    "\n",
    "        # Handle classifiers vs regressors\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values_to_plot = shap_values[1]  # or 0 depending on the class of interest\n",
    "        else:\n",
    "            shap_values_to_plot = shap_values\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.summary_plot(shap_values_to_plot, X_subset,\n",
    "                        feature_names=self.feature_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"SHAP analysis complete!\")\n",
    "\n",
    "    \n",
    "    def pca_visualization(self, X_test, y_test):\n",
    "        \"\"\"PCA visualization of feature clusters\"\"\"\n",
    "        print(\"\\nPCA Visualization...\")\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_test)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_test, \n",
    "                            cmap='RdYlGn_r', alpha=0.6, s=50)\n",
    "        plt.colorbar(scatter, label='Failure Status')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "        plt.title('PCA Visualization of Fault Classes')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Total variance explained: {sum(pca.explained_variance_ratio_)*100:.1f}%\")\n",
    "    \n",
    "    def cross_dataset_validation(self, data1, data2):\n",
    "        \"\"\"Train on one dataset, test on another\"\"\"\n",
    "        print(\"\\nCROSS-DATASET VALIDATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Prepare datasets\n",
    "        X1 = self.scaler.fit_transform(data1[self.feature_names])\n",
    "        y1 = data1['is_failure']\n",
    "        \n",
    "        X2 = self.scaler.transform(data2[self.feature_names])\n",
    "        y2 = data2['is_failure']\n",
    "        \n",
    "        # Train on dataset 1\n",
    "        self.rf_model.fit(X1, y1)\n",
    "        \n",
    "        # Test on dataset 2\n",
    "        y_pred = self.rf_model.predict(X2)\n",
    "        \n",
    "        acc = accuracy_score(y2, y_pred)\n",
    "        f1 = f1_score(y2, y_pred)\n",
    "        \n",
    "        print(f\"Cross-dataset Accuracy: {acc:.4f}\")\n",
    "        print(f\"Cross-dataset F1 Score: {f1:.4f}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return acc, f1\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"ENHANCED MILITARY ROBOTIC SYSTEM MAINTENANCE\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     # Initialize system\n",
    "#     system = MilitaryRoboticMaintenance()\n",
    "    \n",
    "#     # Generate data\n",
    "#     print(\"\\nGenerating synthetic data...\")\n",
    "#     data = system.generate_synthetic_data(n_samples=1000)\n",
    "    \n",
    "#     # Preprocess\n",
    "#     X_train, X_test, y_train, y_test, y_rul_train, y_rul_test = system.preprocess_data(data)\n",
    "    \n",
    "#     # Optimize and train Random Forest\n",
    "#     system.optimize_rf_with_optuna(X_train, y_train, n_trials=30)\n",
    "#     system.train_optimized_rf(X_train, y_train)\n",
    "    \n",
    "#     # Train deep learning models\n",
    "#     history_gru, history_att, X_test_seq = system.train_deep_models(\n",
    "#         X_train, X_test, y_train, y_test, epochs=30)\n",
    "    \n",
    "#     # Comprehensive evaluation\n",
    "#     results = system.evaluate_all_models(X_test, X_test_seq, y_test, n_runs=5)\n",
    "    \n",
    "#     # PR Curves\n",
    "#     system.plot_pr_curves(X_test, X_test_seq, y_test)\n",
    "    \n",
    "#     # RUL Estimation\n",
    "#     rul_model = system.estimate_rul(X_test, y_rul_test)\n",
    "    \n",
    "#     # SHAP Analysis\n",
    "#     system.shap_analysis(X_test)\n",
    "    \n",
    "#     # PCA Visualization\n",
    "#     system.pca_visualization(X_test, y_test)\n",
    "    \n",
    "#     # Cross-dataset validation (using two different samples)\n",
    "#     data2 = system.generate_synthetic_data(n_samples=500)\n",
    "#     system.cross_dataset_validation(data, data2)\n",
    "    \n",
    "#     print(\"\\nALL ANALYSES COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73c054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
